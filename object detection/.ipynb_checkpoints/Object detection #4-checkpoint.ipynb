{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d898345",
   "metadata": {},
   "source": [
    "# Object detection #4\n",
    "sources: https://pyimagesearch.com/2020/07/13/r-cnn-object-detection-with-keras-tensorflow-and-deep-learning/, https://towardsdatascience.com/step-by-step-r-cnn-implementation-from-scratch-in-python-e97101ccde55\n",
    "\n",
    "Implement an RCNN object detector given the techniques we've learnt previously.\n",
    "\n",
    "We will use selective search to generate ROIs then use these ROIs as our training data to classify objects from our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42190521",
   "metadata": {},
   "source": [
    "## Summary\n",
    "RCNN object detection and classification pipeline:\n",
    "\n",
    "1) Build object detection dataset with selective search (calculate IOU on proposed region with ground truth data and add labels to proposed regions)\n",
    "\n",
    "2) Fine-tune classification model on dataset (utilize transfer learning)\n",
    "\n",
    "3) During inference run selective search on input image\n",
    "\n",
    "4) Make predictions on each ROI using fine tuned model, apply NMS and return results\n",
    "\n",
    "*IOU: intersection over union"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d45b59",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfcd12c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618b3811",
   "metadata": {},
   "source": [
    "## NMS\n",
    "non-maxima suppression (NMS) removes redundant bboxes by discarding those that has overlaps above a given threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0bbb1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Felzenszwalb et al.\n",
    "# boxes should be defined as [(start_x, start_y, end_x, end_y),...]\n",
    "def NMS(boxes, overlapThresh):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "    # picked indexes\n",
    "    pick = []\n",
    "    # coords of all bboxes\n",
    "    x1 = boxes[:,0]\n",
    "    y1 = boxes[:,1]\n",
    "    x2 = boxes[:,2]\n",
    "    y2 = boxes[:,3]\n",
    "    # compute area of bboxes\n",
    "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "    # get indexes sorted by bottom right coord\n",
    "    idxs = np.argsort(x1)\n",
    "    # while there are still bboxes not checked\n",
    "    while len(idxs) > 0:\n",
    "        # grab the last idx\n",
    "        last = len(idxs) - 1\n",
    "        i = idxs[last]\n",
    "        pick.append(i) # list of boxes picked (curr is cfm picked as overlapped bboxes are discarded beforehand)\n",
    "        suppress = [] # list of boxes to be deleted\n",
    "        idxs = idxs[:-1]\n",
    "        # compare all bboxes with last\n",
    "        for pos in range(0, last):\n",
    "            j = idxs[pos]\n",
    "            # compare box i and j for overlap\n",
    "            xx1 = max(x1[i], x1[j])\n",
    "            yy1 = max(y1[i], y1[j])\n",
    "            xx2 = min(x2[i], x2[j])\n",
    "            yy2 = min(y2[i], y2[j])\n",
    "            w = max(0, xx2 - xx1 + 1)\n",
    "            h = max(0, yy2 - yy1 + 1)\n",
    "            overlap = float(w * h) / area[j]\n",
    "            # overlap above threshold? Pos to be deleted\n",
    "            if overlap > overlapThresh:\n",
    "                suppress.append(pos)\n",
    "        # delete all indexes from the index list that are in suppression list\n",
    "        idxs = np.delete(idxs, suppress)\n",
    "    # only picked boxes returned\n",
    "    return boxes[pick]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e562d2",
   "metadata": {},
   "source": [
    "## Selective search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59331459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_search(img, print_info=False):\n",
    "    ss = cv2.ximgproc.segmentation.createSelectiveSearchSegmentation()\n",
    "    ss.setBaseImage(img)\n",
    "    # use fast quality\n",
    "    ss.switchToSelectiveSearchFast()\n",
    "    start = time.time()\n",
    "    rects = ss.process()\n",
    "    end = time.time()\n",
    "    if print_info:\n",
    "        print('Selective search took {:.4f}s'.format(end - start))\n",
    "        print('{} total region proposals'.format(len(rects)))\n",
    "    return rects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650db3f",
   "metadata": {},
   "source": [
    "## IOU\n",
    "IOU value = overlap area : non-overlap area. For the dataset building phase we need to get the IOU of the ground truth box with the ROI from selective search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce3c6121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iou(bb1, bb2):\n",
    "    x_1 = max(bb1[0], bb2[0])\n",
    "    y_1 = max(bb1[1], bb2[1])\n",
    "    x_2 = min(bb1[2], bb2[2])\n",
    "    y_2 = min(bb1[3], bb2[3])\n",
    "    # no contact\n",
    "    if x_2 < x_1 or y_2 < y_1:\n",
    "        return 0.0\n",
    "    intersection_area = (x_2 - x_1) * (y2 - y1)\n",
    "    combined_area = (bb1[2] - bb1[0]) * (bb1[3] - bb1[1]) + (bb2[2] - bb2[0]) * (bb2[3] - bb2[1])\n",
    "    return intersection_area / float(combined_area - intersection_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a0c4a6",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9655fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIDTH = 600\n",
    "INPUT_SIZE = (224, 224) # for classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a0805e",
   "metadata": {},
   "source": [
    "## Build object detection dataset\n",
    "We will train our model to detect airplanes using the airplanes dataset.\n",
    "\n",
    "Pre-process and create the dataset for our classifier. In our case we can have 2 classes for each ROI: foreground (airplane) or background. We will set the label of foreground (airplane) as 1 and background as 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32087fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(img, rects, gtvalues):\n",
    "    imout = img.copy()\n",
    "    counter = 0\n",
    "    falsecounter = 0\n",
    "    flag = 0\n",
    "    fflag = 0\n",
    "    bflag = 0\n",
    "    # for each roi\n",
    "    for e, result in enumerate(rects):\n",
    "        # we only retrive first 2000 region proposals\n",
    "        if e < 2000 and flag == 0:\n",
    "            # for each gt bbox\n",
    "            for gtval in gtvalues:\n",
    "                (x, y, w, h) = result\n",
    "                iou = get_iou(gtval, (x, y, x + w, y + h))\n",
    "                # limit to 30 valid ROIs\n",
    "                if counter < 30:\n",
    "                    if iou > 0.7:\n",
    "                        timage = imout[y:y + h, x:x + w]\n",
    "                        resized = cv2.resize(timage, INPUT_SIZE, interpolation = cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e899dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selective search took 0.4151s\n",
      "308 total region proposals\n"
     ]
    }
   ],
   "source": [
    "x_train = []\n",
    "y_train = []\n",
    "iter_count = 0\n",
    "for e, i in enumerate(os.listdir('data/Airplanes_Annotations')):\n",
    "    if iter_count == 1:\n",
    "        break\n",
    "    if i.startswith('airplane'):\n",
    "        # extract image and annotation\n",
    "        filename = i.split('.')[0] + '.jpg'\n",
    "        img = cv2.imread('data/Images/' + filename)\n",
    "        img = np.flip(img, axis=-1)\n",
    "        df = pd.read_csv('data/Airplanes_Annotations/' + i)\n",
    "        # format annots\n",
    "        gtvalues=[]\n",
    "        for row in df.iterrows():\n",
    "            coords = row[1][0].split(' ')\n",
    "            # store annotated gt coords\n",
    "            gtvalues.append(tuple([int(x) for x in coords]))\n",
    "        # apply selective search\n",
    "        rects = selective_search(img, True)\n",
    "        # process data with IOU between gt and roi\n",
    "        \n",
    "        iter_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f312ecf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
