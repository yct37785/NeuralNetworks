{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c0330c",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "CNN implementation with Tensorflow\n",
    "\n",
    "Sources:\n",
    "https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4075ef2",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cec33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e8f2b",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "We will use the CIFAR10 dataset containing 60k color images in 10 classes, 6k per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# normalize pixel values\n",
    "X_train = X_train.astype(float) / 255.\n",
    "X_test = X_test.astype(float) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425e8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98848e20",
   "metadata": {},
   "source": [
    "## Visualize dataset\n",
    "Note that unlike mnist, each image is 32 x 32 and has 3 channels (rgb), hence 32 x 32 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aeb306",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Feature matrix:', X_train.shape)\n",
    "print('Target matrix:', X_test.shape)\n",
    "print('Feature matrix:', y_train.shape)\n",
    "print('Target matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ae0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X_train[i])\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[y_train[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad1c51",
   "metadata": {},
   "source": [
    "## Create our model\n",
    "Our model will be processing images with RGB color channels hence each input image is of 32 x 32 x 3 dimension. Our convolutional base uses a common pattern: a stack of Conv2D and MaxPooling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6278e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    Dropout(.2),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    #\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    Dropout(.2),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    #\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb951f6",
   "metadata": {},
   "source": [
    "Conv2D params:\n",
    "- filters: dimensionality of the output space (how many output filters in the convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7456156",
   "metadata": {},
   "source": [
    "View the architecture of the model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71298861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e53a10",
   "metadata": {},
   "source": [
    "Note that all layers do not have padding applied hence the loss of 2 layers for a kernel of size 3 x 3.\n",
    "\n",
    "The output of every Conv2D and MaxPooling2D is a 3D tensor of shape (height, width, channels). Number of output channels is controlled by the filter argument in Conv2D which specifies how many filters are applied to this layer. i.e. 32 filters = 32 channels.\n",
    "\n",
    "As width and height shrinks it will be more computationally affordable to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d7079",
   "metadata": {},
   "source": [
    "## Classification\n",
    "To complete the model, we will feed the last output tensor (4,4,64) into one more more dense layers to perform classification. Flatten the 3D tensor into 1D before feeding it into the first dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9f292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ba6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeecd01",
   "metadata": {},
   "source": [
    "## Complete model\n",
    "Compile with optimizer (Adam, SGD etc) and loss calculation (MSE etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f47082",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d769a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0f1f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "          epochs=10, \n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00a699",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "accuracy: training data accuracy\n",
    "\n",
    "val_accuracy: validation split accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c570598",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915be5fc",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9da29a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978a0f8",
   "metadata": {},
   "source": [
    "## Results analysis\n",
    "Default model from TensorFlow tutorial: 0.71"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
