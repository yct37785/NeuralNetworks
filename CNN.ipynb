{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c0330c",
   "metadata": {},
   "source": [
    "# CNN\n",
    "\n",
    "CNN implementation with Tensorflow\n",
    "\n",
    "Sources:\n",
    "https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4075ef2",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20cec33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Activation\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88e8f2b",
   "metadata": {},
   "source": [
    "## Load dataset\n",
    "We will use the CIFAR10 dataset containing 60k color images in 10 classes, 6k per class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff4c45f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2dccc",
   "metadata": {},
   "source": [
    "MNIST dataset option (comment out CIFAR10 cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a808b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "# reshape to have a single channel\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97580f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize pixel values\n",
    "X_train = X_train.astype(float) / 255.\n",
    "X_test = X_test.astype(float) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425e8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98848e20",
   "metadata": {},
   "source": [
    "## Visualize dataset\n",
    "Each image is 32 x 32 and has 3 channels (rgb), hence 32 x 32 x 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08aeb306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix: (60000, 28, 28, 1)\n",
      "Target matrix: (10000, 28, 28, 1)\n",
      "Feature matrix: (60000,)\n",
      "Target matrix: (10000,)\n"
     ]
    }
   ],
   "source": [
    "print('Feature matrix:', X_train.shape)\n",
    "print('Target matrix:', X_test.shape)\n",
    "print('Feature matrix:', y_train.shape)\n",
    "print('Target matrix:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12ae0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,10))\n",
    "# for i in range(25):\n",
    "#     plt.subplot(5,5,i+1)\n",
    "#     plt.xticks([])\n",
    "#     plt.yticks([])\n",
    "#     plt.grid(False)\n",
    "#     plt.imshow(X_train[i])\n",
    "#     # The CIFAR labels happen to be arrays, \n",
    "#     # which is why you need the extra index\n",
    "#     plt.xlabel(class_names[y_train[i][0]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbad1c51",
   "metadata": {},
   "source": [
    "## Create our model\n",
    "Our model will be processing images with RGB color channels hence each input image is of 32 x 32 x 3 dimension. Our convolutional base uses a common pattern: a stack of Conv2D and MaxPooling2D layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6278e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    #\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=X_train[0].shape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(.2),\n",
    "    #\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(.1),\n",
    "    #\n",
    "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb951f6",
   "metadata": {},
   "source": [
    "Conv2D params:\n",
    "- filters: dimensionality of the output space (how many output filters in the convolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7456156",
   "metadata": {},
   "source": [
    "View the architecture of the model so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71298861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e53a10",
   "metadata": {},
   "source": [
    "Note that all layers do not have padding applied hence the loss of 2 layers for a kernel of size 3 x 3.\n",
    "\n",
    "The output of every Conv2D and MaxPooling2D is a 3D tensor of shape (height, width, channels). Number of output channels is controlled by the filter argument in Conv2D which specifies how many filters are applied to this layer. i.e. 32 filters = 32 channels.\n",
    "\n",
    "As width and height shrinks it will be more computationally affordable to add more output channels in each Conv2D layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5d7079",
   "metadata": {},
   "source": [
    "## Classification\n",
    "To complete the model, we will feed the last output tensor (4,4,64) into one more more dense layers to perform classification. Flatten the 3D tensor into 1D before feeding it into the first dense layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c9f292f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0ba6079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeecd01",
   "metadata": {},
   "source": [
    "## Complete model\n",
    "Compile with optimizer (Adam, SGD etc) and loss calculation (MSE etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f47082",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='sparse_categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3d769a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c0f1f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.1888 - accuracy: 0.9405 - val_loss: 0.0700 - val_accuracy: 0.9791\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 18s 12ms/step - loss: 0.0551 - accuracy: 0.9823 - val_loss: 0.0575 - val_accuracy: 0.9840\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.0421 - accuracy: 0.9869 - val_loss: 0.0410 - val_accuracy: 0.9879\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0348 - accuracy: 0.9887 - val_loss: 0.0373 - val_accuracy: 0.9895\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.0277 - accuracy: 0.9911 - val_loss: 0.0444 - val_accuracy: 0.9879\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 0.0383 - val_accuracy: 0.9890\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0207 - accuracy: 0.9931 - val_loss: 0.0396 - val_accuracy: 0.9891\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0193 - accuracy: 0.9936 - val_loss: 0.0336 - val_accuracy: 0.9912\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 19s 13ms/step - loss: 0.0158 - accuracy: 0.9948 - val_loss: 0.0394 - val_accuracy: 0.9903\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 19s 12ms/step - loss: 0.0137 - accuracy: 0.9954 - val_loss: 0.0331 - val_accuracy: 0.9920\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=10, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00a699",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "accuracy: training data accuracy\n",
    "\n",
    "val_accuracy: validation split accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c570598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x280d5d8d370>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915be5fc",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9da29a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss, test acc: [0.0307389535009861, 0.9919000267982483]\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b978a0f8",
   "metadata": {},
   "source": [
    "## Results analysis\n",
    "CIFAR10\n",
    "* Default model from TensorFlow tutorial: 0.71\n",
    "\n",
    "MNIST\n",
    "* Default model from TensorFlow tutorial: 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd10d89",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4befa483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yctan\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\yctan\\.conda\\envs\\ML\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: CNN_mnist\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"CNN_mnist\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
